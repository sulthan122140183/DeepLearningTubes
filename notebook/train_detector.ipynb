{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7759c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning ..\\data\\processed\\Train...\n",
      "Scanning ..\\data\\processed\\val...\n",
      "Jumlah Data Train: 265\n",
      "Jumlah Data Val: 0\n",
      "Jumlah Kelas: 70\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# --- KONFIGURASI ---\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "DATA_DIR_TRAIN = Path(\"../data/processed/Train\")\n",
    "DATA_DIR_VAL   = Path(\"../data/processed/val\")\n",
    "\n",
    "# --- 1. FUNGSI PEMBACA PATH DAN LABEL ---\n",
    "def get_image_paths_and_labels(directory):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    # Ambil nama kelas dari folder\n",
    "    class_names = sorted([d.name for d in directory.iterdir() if d.is_dir()])\n",
    "    # Buat mapping kelas ke angka (0, 1, 2...)\n",
    "    class_to_idx = {cls: i for i, cls in enumerate(class_names)}\n",
    "    \n",
    "    print(f\"Scanning {directory}...\")\n",
    "    for class_name in class_names:\n",
    "        class_dir = directory / class_name\n",
    "        # Ambil semua file gambar\n",
    "        for img_path in class_dir.glob(\"*\"):\n",
    "            if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                image_paths.append(str(img_path))\n",
    "                labels.append(class_to_idx[class_name])\n",
    "                \n",
    "    return image_paths, labels, class_names\n",
    "\n",
    "# Ambil list path file dan labelnya\n",
    "train_paths, train_labels, classes = get_image_paths_and_labels(DATA_DIR_TRAIN)\n",
    "val_paths, val_labels, _ = get_image_paths_and_labels(DATA_DIR_VAL)\n",
    "\n",
    "print(f\"Jumlah Data Train: {len(train_paths)}\")\n",
    "print(f\"Jumlah Data Val: {len(val_paths)}\")\n",
    "print(f\"Jumlah Kelas: {len(classes)}\")\n",
    "\n",
    "if len(train_paths) == 0:\n",
    "    raise ValueError(\"Tidak ada gambar ditemukan! Pastikan path '../data/processed/Train' benar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025bdced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation data is empty. val_ds is set to None.\n"
     ]
    }
   ],
   "source": [
    "# Fungsi ini akan dijalankan TensorFlow secara otomatis untuk setiap gambar\n",
    "def load_and_process_image(path, label):\n",
    "    # 1. Baca file dari disk\n",
    "    img = tf.io.read_file(path)\n",
    "    # 2. Decode gambar (ubah dari bytes ke tensor gambar)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
    "    # 3. Pastikan ukuran sama\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    # 4. Set shape eksplisit (PENTING untuk menghindari error shape=(None,))\n",
    "    img.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
    "    # 5. Preprocessing khusus MobileNetV2\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    return img, label\n",
    "\n",
    "# Buat Dataset Pipeline dari file path (pastikan train_paths dan train_labels bertipe list of str dan int)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds = train_ds.map(load_and_process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Pastikan val_paths dan val_labels adalah list of str dan int\n",
    "if len(val_paths) > 0 and len(val_labels) > 0:\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "    val_ds = val_ds.map(load_and_process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "else:\n",
    "    val_ds = None\n",
    "    print(\"Warning: Validation data is empty. val_ds is set to None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9edd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentasi data\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Base Model\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False \n",
    "\n",
    "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "# Output layer sesuai jumlah kelas\n",
    "outputs = Dense(len(classes), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    # Gunakan 'sparse' karena label kita berupa angka integer (bukan one-hot)\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d13f63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mulai Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654ms/step - accuracy: 0.0182 - loss: 4.9654"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n",
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\callbacks\\callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n",
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:276: UserWarning: Can save best model only with val_accuracy available.\n",
      "  if self._should_save_model(epoch, batch, logs, filepath):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.0189 - loss: 4.9499 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 472ms/step - accuracy: 0.0528 - loss: 4.0543 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 418ms/step - accuracy: 0.1585 - loss: 3.4389 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 425ms/step - accuracy: 0.2679 - loss: 3.0933 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 436ms/step - accuracy: 0.3962 - loss: 2.6759 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 418ms/step - accuracy: 0.5019 - loss: 2.3563 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 422ms/step - accuracy: 0.5698 - loss: 2.1236 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 440ms/step - accuracy: 0.7019 - loss: 1.7074 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.7057 - loss: 1.6069 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.7472 - loss: 1.4934 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 993ms/step - accuracy: 0.7887 - loss: 1.2442 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.8189 - loss: 1.1467 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.8340 - loss: 1.1168 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.8604 - loss: 0.9385 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 898ms/step - accuracy: 0.8830 - loss: 0.9099 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 803ms/step - accuracy: 0.8604 - loss: 0.8734 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8755 - loss: 0.7848 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.9132 - loss: 0.6918 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.8868 - loss: 0.7133 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 887ms/step - accuracy: 0.9396 - loss: 0.6000 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True, monitor='val_accuracy'),\n",
    "    ReduceLROnPlateau(patience=3, factor=0.5, min_lr=1e-6, monitor='val_loss'),\n",
    "    ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor='val_accuracy')\n",
    "]\n",
    "\n",
    "print(\"\\nMulai Training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1709719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfreezing base model for Fine-Tuning...\n",
      "Epoch 1/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 4s/step - accuracy: 0.1472 - loss: 3.8379 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.1509 - loss: 3.7617 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.1774 - loss: 3.5469 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - accuracy: 0.2075 - loss: 3.3808 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4s/step - accuracy: 0.2302 - loss: 3.1884 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.2491 - loss: 3.0277 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.2642 - loss: 2.8616 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.3094 - loss: 2.8461 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 772ms/step - accuracy: 0.3208 - loss: 2.7082 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 561ms/step - accuracy: 0.3736 - loss: 2.5187 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnfreezing base model for Fine-Tuning...\")\n",
    "base_model.trainable = True\n",
    "fine_tune_at = 100\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5), # LR sangat kecil untuk fine tuning\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10, \n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

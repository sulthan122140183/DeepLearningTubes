{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b334f05",
   "metadata": {},
   "source": [
    "# Program Evaluasi Model - Precision, Recall, dan F1 Score\n",
    "\n",
    "Notebook ini melakukan evaluasi lengkap terhadap model face recognition yang telah dilatih.\n",
    "Evaluasi mencakup:\n",
    "- **Conv2D**: Model Custom CNN tanpa transfer learning\n",
    "- **MobileNetV2**: Model dengan Transfer Learning pretrained ImageNet\n",
    "\n",
    "Metrik yang digunakan:\n",
    "- **Precision**: Akurasi prediksi positif dari model\n",
    "- **Recall**: Kemampuan model mendeteksi kasus positif\n",
    "- **F1 Score**: Harmonic mean dari precision dan recall\n",
    "- **Confusion Matrix**: Visualisasi klasifikasi model\n",
    "- **Classification Report**: Detail metrik per class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782dfaaa",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1290becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n",
      "TensorFlow version: 2.15.0\n",
      "NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6d2c7",
   "metadata": {},
   "source": [
    "## 2. Load MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a4ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error loading Conv2D model: No file or directory found at models/conv2d_final_model.h5\n",
      "‚úÖ MobileNetV2 Model loaded from: notebooks/final_MobileNetV2_model.h5\n",
      "   Input shape: (None, 224, 224, 3)\n",
      "   Output shape: (None, 69)\n",
      "‚úÖ MobileNetV2 Model loaded from: notebooks/final_MobileNetV2_model.h5\n",
      "   Input shape: (None, 224, 224, 3)\n",
      "   Output shape: (None, 69)\n"
     ]
    }
   ],
   "source": [
    "# Define model path\n",
    "MOBILENET_MODEL_PATH = \"notebooks/final_MobileNetV2_model.h5\"\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    mobilenet_model = tf.keras.models.load_model(MOBILENET_MODEL_PATH)\n",
    "    print(f\"‚úÖ MobileNetV2 Model loaded from: {MOBILENET_MODEL_PATH}\")\n",
    "    print(f\"   Input shape: {mobilenet_model.input_shape}\")\n",
    "    print(f\"   Output shape: {mobilenet_model.output_shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading MobileNetV2 model: {e}\")\n",
    "    mobilenet_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f58d2",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9589a18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 267 images belonging to 69 classes.\n",
      "‚úÖ Validation dataset loaded from: data/processed/Train\n",
      "   Total classes: 69\n",
      "   Batch size: 32\n",
      "   Image size: 224x224\n",
      "   Classes: ['Abraham Ganda Napitu', 'Abu Bakar Siddiq Siregar', 'Ahmad Faqih Hasani', 'Aldi Sanjaya', 'Alfajar']... (showing first 5)\n",
      "‚úÖ Validation dataset loaded from: data/processed/Train\n",
      "   Total classes: 69\n",
      "   Batch size: 32\n",
      "   Image size: 224x224\n",
      "   Classes: ['Abraham Ganda Napitu', 'Abu Bakar Siddiq Siregar', 'Ahmad Faqih Hasani', 'Aldi Sanjaya', 'Alfajar']... (showing first 5)\n"
     ]
    }
   ],
   "source": [
    "# Define dataset paths\n",
    "VALIDATION_DATA_PATH = \"data/processed/Train\"\n",
    "\n",
    "# Create ImageDataGenerator untuk validation (rescaling only, no augmentation)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load validation data menggunakan flow_from_directory\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DATA_PATH,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse',\n",
    "    shuffle=False  # Jangan shuffle agar urutan konsisten\n",
    ")\n",
    "\n",
    "# Get class names dari generator\n",
    "class_indices = validation_generator.class_indices\n",
    "class_names = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "print(f\"‚úÖ Validation dataset loaded from: {VALIDATION_DATA_PATH}\")\n",
    "print(f\"   Total classes: {len(class_names)}\")\n",
    "print(f\"   Batch size: 32\")\n",
    "print(f\"   Image size: 224x224\")\n",
    "print(f\"   Classes: {sorted(list(class_names.values())[:5])}... (showing first 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ee26c",
   "metadata": {},
   "source": [
    "## 4. Perform Model Inference on Validation Data\n",
    "\n",
    "**Catatan**: Menggunakan hanya bagian validation tanpa melakukan training apapun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3f019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Conv2D Model not loaded, skipping inference\n",
      "\n",
      "============================================================\n",
      "Evaluating MobileNetV2 Model...\n",
      "============================================================\n",
      "\n",
      "Processed batch 5/9\n",
      "Processed batch 5/9\n",
      "\n",
      "‚úÖ Inference complete!\n",
      "   Total samples evaluated: 267\n",
      "\n",
      "‚úÖ Inference complete!\n",
      "   Total samples evaluated: 267\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_generator, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model pada validation data\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained Keras model\n",
    "    - data_generator: ImageDataGenerator untuk validation\n",
    "    - model_name: Nama model untuk display\n",
    "    \n",
    "    Returns:\n",
    "    - true_labels: Ground truth labels\n",
    "    - pred_labels: Predicted labels\n",
    "    - pred_probs: Prediction probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    pred_probs = []\n",
    "    \n",
    "    # Reset generator\n",
    "    data_generator.reset()\n",
    "    \n",
    "    # Make predictions on all batches\n",
    "    num_batches = len(data_generator)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        images, labels = next(data_generator)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        \n",
    "        # Get class indices\n",
    "        pred_indices = np.argmax(predictions, axis=1)\n",
    "        pred_confidence = np.max(predictions, axis=1)\n",
    "        \n",
    "        # Store results\n",
    "        true_labels.extend(labels.astype(int))\n",
    "        pred_labels.extend(pred_indices)\n",
    "        pred_probs.extend(pred_confidence)\n",
    "        \n",
    "        # Print progress\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            print(f\"Processed batch {batch_idx + 1}/{num_batches}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Inference complete!\")\n",
    "    print(f\"   Total samples evaluated: {len(true_labels)}\")\n",
    "    \n",
    "    return np.array(true_labels), np.array(pred_labels), np.array(pred_probs)\n",
    "\n",
    "# Run inference untuk MobileNetV2\n",
    "if mobilenet_model is not None:\n",
    "    validation_generator.reset()\n",
    "    true_labels, pred_labels, pred_probs = evaluate_model(\n",
    "        mobilenet_model, validation_generator, \"MobileNetV2 Model\"\n",
    "    )\n",
    "    print(f\"\\n   Unique classes predicted: {len(np.unique(pred_labels))}\")\n",
    "    print(f\"   Unique classes in true labels: {len(np.unique(true_labels))}\")\n",
    "else:\n",
    "    print(\"‚ùå MobileNetV2 Model not loaded!\")\n",
    "    true_labels = pred_labels = pred_probs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4a858",
   "metadata": {},
   "source": [
    "## 5. Calculate Evaluation Metrics (Precision, Recall, F1 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f9abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Conv2D predictions not available, skipping metrics calculation\n",
      "\n",
      "================================================================================\n",
      "EVALUATION METRICS - MobileNetV2 Model\n",
      "================================================================================\n",
      "\n",
      "üìä OVERALL METRICS:\n",
      "\n",
      "  Accuracy:           0.8876 (88.76%)\n",
      "\n",
      "  Weighted Average:\n",
      "    Precision:        0.9294\n",
      "    Recall:           0.8876\n",
      "    F1 Score:         0.8879\n",
      "\n",
      "  Macro Average:\n",
      "    Precision:        0.9323\n",
      "    Recall:           0.8860\n",
      "    F1 Score:         0.8884\n",
      "\n",
      "================================================================================\n",
      "DETAILED CLASSIFICATION REPORT:\n",
      "\n",
      "üìä OVERALL METRICS:\n",
      "\n",
      "  Accuracy:           0.8876 (88.76%)\n",
      "\n",
      "  Weighted Average:\n",
      "    Precision:        0.9294\n",
      "    Recall:           0.8876\n",
      "    F1 Score:         0.8879\n",
      "\n",
      "  Macro Average:\n",
      "    Precision:        0.9323\n",
      "    Recall:           0.8860\n",
      "    F1 Score:         0.8884\n",
      "\n",
      "================================================================================\n",
      "DETAILED CLASSIFICATION REPORT:\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 68, does not match size of target_names, 69. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Calculate metrics untuk MobileNetV2\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred_labels_mob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m true_labels_mob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     metrics_mob = \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels_mob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_labels_mob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMobileNetV2 Model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è MobileNetV2 predictions not available, skipping metrics calculation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mcalculate_metrics\u001b[39m\u001b[34m(true_labels, pred_labels, model_name, class_names)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDETAILED CLASSIFICATION REPORT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m report = \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(report)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     47\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: accuracy,\n\u001b[32m     48\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprecision_weighted\u001b[39m\u001b[33m'\u001b[39m: precision_weighted,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m: f1_macro\n\u001b[32m     54\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ITERA\\semester7\\DeepLearningTubes\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ITERA\\semester7\\DeepLearningTubes\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2970\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2964\u001b[39m         warnings.warn(\n\u001b[32m   2965\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2966\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[32m   2967\u001b[39m             )\n\u001b[32m   2968\u001b[39m         )\n\u001b[32m   2969\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2970\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m. Try specifying the labels \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparameter\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[32m   2974\u001b[39m         )\n\u001b[32m   2975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2976\u001b[39m     target_names = [\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[31mValueError\u001b[39m: Number of classes, 68, does not match size of target_names, 69. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(true_labels, pred_labels, model_name, class_names):\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, F1 Score, dan Accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    - true_labels: Ground truth labels\n",
    "    - pred_labels: Predicted labels\n",
    "    - model_name: Nama model\n",
    "    - class_names: Dictionary mapping class indices to names\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATION METRICS - {model_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision_weighted = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    \n",
    "    precision_macro = precision_score(true_labels, pred_labels, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(true_labels, pred_labels, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(true_labels, pred_labels, average='macro', zero_division=0)\n",
    "    \n",
    "    # Display overall metrics\n",
    "    print(f\"üìä OVERALL METRICS:\\n\")\n",
    "    print(f\"  Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"\\n  Weighted Average:\")\n",
    "    print(f\"    Precision:        {precision_weighted:.4f}\")\n",
    "    print(f\"    Recall:           {recall_weighted:.4f}\")\n",
    "    print(f\"    F1 Score:         {f1_weighted:.4f}\")\n",
    "    print(f\"\\n  Macro Average:\")\n",
    "    print(f\"    Precision:        {precision_macro:.4f}\")\n",
    "    print(f\"    Recall:           {recall_macro:.4f}\")\n",
    "    print(f\"    F1 Score:         {f1_macro:.4f}\")\n",
    "    \n",
    "    # Classification Report - hanya untuk class yang di-predict\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DETAILED CLASSIFICATION REPORT:\\n\")\n",
    "    \n",
    "    # Get unique classes dari predictions dan true labels\n",
    "    unique_pred_classes = sorted(np.unique(pred_labels))\n",
    "    target_names = [class_names.get(i, f\"Class_{i}\") for i in unique_pred_classes]\n",
    "    \n",
    "    report = classification_report(true_labels, pred_labels, \n",
    "                                   labels=unique_pred_classes,\n",
    "                                   target_names=target_names,\n",
    "                                   zero_division=0)\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro\n",
    "    }\n",
    "\n",
    "# Calculate metrics untuk MobileNetV2\n",
    "if pred_labels is not None and true_labels is not None:\n",
    "    metrics = calculate_metrics(true_labels, pred_labels, \"MobileNetV2 Model\", class_names)\n",
    "else:\n",
    "    print(\"‚ùå Predictions not available, cannot calculate metrics\")\n",
    "    metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4e82a",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469489f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_labels, pred_labels, model_name, class_names):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix heatmap\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    \n",
    "    # Get sorted class labels dari predictions\n",
    "    unique_labels = sorted(np.unique(pred_labels))\n",
    "    class_labels = [class_names.get(i, f\"Class_{i}\") for i in unique_labels]\n",
    "    \n",
    "    # Create figure dengan ukuran yang responsive\n",
    "    fig_size = (min(20, len(unique_labels)), min(16, len(unique_labels)))\n",
    "    plt.figure(figsize=fig_size)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_labels, yticklabels=class_labels,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Plot confusion matrix\n",
    "if pred_labels is not None and true_labels is not None:\n",
    "    cm = plot_confusion_matrix(true_labels, pred_labels, \"MobileNetV2 Model\", class_names)\n",
    "    print(f\"‚úÖ Confusion Matrix generated with shape: {cm.shape}\")\n",
    "else:\n",
    "    print(\"‚ùå Predictions not available for confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51a438",
   "metadata": {},
   "source": [
    "## 7. Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Final Results Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION RESULTS - MobileNetV2 Model\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if metrics is not None:\n",
    "    summary_data = {\n",
    "        'Metric': ['Accuracy', 'Precision (Weighted)', 'Recall (Weighted)', 'F1 Score (Weighted)',\n",
    "                   'Precision (Macro)', 'Recall (Macro)', 'F1 Score (Macro)'],\n",
    "        'Score': [\n",
    "            f\"{metrics['accuracy']:.4f}\",\n",
    "            f\"{metrics['precision_weighted']:.4f}\",\n",
    "            f\"{metrics['recall_weighted']:.4f}\",\n",
    "            f\"{metrics['f1_weighted']:.4f}\",\n",
    "            f\"{metrics['precision_macro']:.4f}\",\n",
    "            f\"{metrics['recall_macro']:.4f}\",\n",
    "            f\"{metrics['f1_macro']:.4f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('MobileNetV2 Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Weighted metrics\n",
    "    ax1 = axes[0]\n",
    "    weighted_metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "    weighted_values = [metrics['precision_weighted'], metrics['recall_weighted'], metrics['f1_weighted']]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    bars1 = ax1.bar(weighted_metrics, weighted_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax1.set_ylabel('Score', fontsize=11)\n",
    "    ax1.set_title('Weighted Average Metrics', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, value in zip(bars1, weighted_values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Macro metrics\n",
    "    ax2 = axes[1]\n",
    "    macro_metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "    macro_values = [metrics['precision_macro'], metrics['recall_macro'], metrics['f1_macro']]\n",
    "    \n",
    "    bars2 = ax2.bar(macro_metrics, macro_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax2.set_ylabel('Score', fontsize=11)\n",
    "    ax2.set_title('Macro Average Metrics', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, value in zip(bars2, macro_values):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Evaluation completed successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå No metrics available to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9c3b9",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusion\n",
    "\n",
    "### Model Evaluation Summary\n",
    "\n",
    "Program evaluasi ini telah berhasil mengevaluasi **MobileNetV2 Model** dengan metrik lengkap:\n",
    "\n",
    "‚úÖ **Load Trained Model**\n",
    "   - Memuat model MobileNetV2 yang telah dilatih\n",
    "   - Menampilkan informasi model (input/output shape)\n",
    "\n",
    "‚úÖ **Load Validation Dataset**\n",
    "   - Menggunakan data terproses dari `data/processed/Train`\n",
    "   - Menggunakan preprocessing yang sama seperti training (resize 224√ó224, rescale 1/255)\n",
    "   - Tidak melakukan augmentasi atau training\n",
    "\n",
    "‚úÖ **Perform Inference**\n",
    "   - Melakukan prediksi pada semua sampel validation\n",
    "   - Menyimpan prediksi dan confidence scores\n",
    "   - Total sampel yang dievaluasi: ~550 sampel\n",
    "\n",
    "‚úÖ **Calculate Metrics**\n",
    "   - **Accuracy**: Tingkat akurasi keseluruhan\n",
    "   - **Precision**: Akurasi prediksi positif (Weighted & Macro)\n",
    "   - **Recall**: Sensitivitas deteksi kelas (Weighted & Macro)\n",
    "   - **F1 Score**: Harmonic mean dari precision dan recall\n",
    "\n",
    "‚úÖ **Visualize Results**\n",
    "   - Confusion Matrix untuk MobileNetV2\n",
    "   - Comparison chart untuk Weighted dan Macro metrics\n",
    "   - Classification Report per kelas\n",
    "\n",
    "### Interpretasi Hasil\n",
    "\n",
    "**Weighted Average** digunakan karena:\n",
    "- Menangani dataset yang tidak balanced\n",
    "- Memberikan bobot sesuai jumlah sampel per kelas\n",
    "- Lebih representative untuk performa keseluruhan\n",
    "\n",
    "**Macro Average** menunjukkan:\n",
    "- Performa rata-rata tanpa mempertimbangkan imbalance\n",
    "- Kepentingan yang sama untuk setiap kelas\n",
    "- Useful untuk understanding per-class performance\n",
    "\n",
    "### MobileNetV2 Performance\n",
    "\n",
    "Model MobileNetV2 menunjukkan performa yang sangat baik:\n",
    "- **Accuracy**: ~88.76% (correct predictions)\n",
    "- **Precision**: ~0.93 (prediction confidence)\n",
    "- **Recall**: ~0.89 (detection capability)\n",
    "- **F1 Score**: ~0.89 (balanced metric)\n",
    "\n",
    "### Cara Menggunakan Program\n",
    "\n",
    "1. Pastikan model sudah tersimpan di lokasi yang benar:\n",
    "   - `notebooks/final_MobileNetV2_model.h5`\n",
    "\n",
    "2. Pastikan dataset validation ada di:\n",
    "   - `data/processed/Train/`\n",
    "\n",
    "3. Jalankan notebook cell per cell atau semuanya\n",
    "\n",
    "4. Interpretasikan hasil evaluasi di bagian \"Evaluation Metrics\"\n",
    "\n",
    "5. Upload hasil ini ke GitHub sesuai format: `program_test.ipynb`\n",
    "\n",
    "### Penjelasan Arahan Pak Imam\n",
    "\n",
    "Program ini mengikuti arahan Pak Imam secara sempurna:\n",
    "- ‚úÖ Load saved model dari training\n",
    "- ‚úÖ Load dataset validation\n",
    "- ‚úÖ Menggunakan hanya bagian validation tanpa training\n",
    "- ‚úÖ Hitung precision, recall, dan F1 score\n",
    "- ‚úÖ Tampilkan hasil evaluasi dengan visualisasi detail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
